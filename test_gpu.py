#!/usr/bin/env python3
"""
GPU è¯Šæ–­è„šæœ¬ - åˆ†æ­¥æµ‹è¯•å„ä¸ªç»„ä»¶
"""
import os
import time

print("=" * 60)
print("Step 1: æ£€æŸ¥ JAX é…ç½®")
print("=" * 60)

import jax
import jax.numpy as jnp

print(f"JAX ç‰ˆæœ¬: {jax.__version__}")
print(f"é»˜è®¤åç«¯: {jax.default_backend()}")
print(f"å¯ç”¨è®¾å¤‡: {jax.devices()}")

if jax.default_backend() != 'gpu':
    print("\nâš ï¸ è­¦å‘Š: JAX æ²¡æœ‰ä½¿ç”¨ GPU åç«¯!")
    print("è¯·ç¡®ä¿å®‰è£…äº† jax[cuda] ç‰ˆæœ¬")
    print("pip install --upgrade 'jax[cuda12]'")

print("\n" + "=" * 60)
print("Step 2: æµ‹è¯•ç®€å• GPU è®¡ç®—")
print("=" * 60)

# ç®€å•çŸ©é˜µä¹˜æ³•æµ‹è¯•
@jax.jit
def simple_matmul(x, y):
    return jnp.dot(x, y)

x = jnp.ones((1000, 1000))
y = jnp.ones((1000, 1000))

print("ç¼–è¯‘ç®€å•çŸ©é˜µä¹˜æ³•...")
t0 = time.time()
result = simple_matmul(x, y)
result.block_until_ready()
print(f"é¦–æ¬¡è¿è¡Œ (å«ç¼–è¯‘): {time.time() - t0:.2f}s")

t0 = time.time()
for _ in range(10):
    result = simple_matmul(x, y)
result.block_until_ready()
print(f"10 æ¬¡è¿è¡Œ: {time.time() - t0:.4f}s")
print("âœ… ç®€å• GPU è®¡ç®—æ­£å¸¸")

print("\n" + "=" * 60)
print("Step 3: æµ‹è¯•ç¯å¢ƒåˆå§‹åŒ–")
print("=" * 60)

from xiangqi.env import XiangqiEnv

env = XiangqiEnv()
print(f"åŠ¨ä½œç©ºé—´å¤§å°: {env.action_space_size}")
print(f"è§‚å¯Ÿå½¢çŠ¶: {env.observation_shape}")

key = jax.random.PRNGKey(42)

print("ç¼–è¯‘å•å±€åˆå§‹åŒ–...")
t0 = time.time()
state = env.init(key)
print(f"å•å±€åˆå§‹åŒ–: {time.time() - t0:.2f}s")

print("\nç¼–è¯‘ vmap åˆå§‹åŒ– (16 å±€)...")
v_init = jax.vmap(env.init)
t0 = time.time()
keys = jax.random.split(key, 16)
states = v_init(keys)
print(f"vmap åˆå§‹åŒ–: {time.time() - t0:.2f}s")
print("âœ… ç¯å¢ƒåˆå§‹åŒ–æ­£å¸¸")

print("\n" + "=" * 60)
print("Step 4: æµ‹è¯•ç¯å¢ƒ step")
print("=" * 60)

print("ç¼–è¯‘å•å±€ step...")
t0 = time.time()
# æ‰¾ä¸€ä¸ªåˆæ³•åŠ¨ä½œ
action = jnp.argmax(state.legal_action_mask)
new_state = env.step(state, action)
print(f"å•å±€ step: {time.time() - t0:.2f}s")

print("\nç¼–è¯‘ vmap step (16 å±€)...")
v_step = jax.vmap(env.step)
t0 = time.time()
actions = jnp.argmax(states.legal_action_mask, axis=-1)
new_states = v_step(states, actions)
print(f"vmap step: {time.time() - t0:.2f}s")
print("âœ… ç¯å¢ƒ step æ­£å¸¸")

print("\n" + "=" * 60)
print("Step 5: æµ‹è¯•ç½‘ç»œå‰å‘ä¼ æ’­")
print("=" * 60)

from networks.muzero import MuZeroNetwork, create_train_state

network = MuZeroNetwork(
    action_space_size=env.action_space_size,
    hidden_dim=128,  # ç”¨è¾ƒå°çš„éšè—å±‚
)

print("åˆå§‹åŒ–ç½‘ç»œ...")
t0 = time.time()
train_state = create_train_state(
    key, network,
    input_shape=(16, 240, 10, 9),
    learning_rate=0.001,
)
print(f"ç½‘ç»œåˆå§‹åŒ–: {time.time() - t0:.2f}s")

print("\nç¼–è¯‘ç½‘ç»œå‰å‘ä¼ æ’­...")
v_observe = jax.vmap(env.observe)
obs = v_observe(states)

@jax.jit
def forward(params, obs):
    return network.apply(params, obs)

t0 = time.time()
output = forward(train_state.params, obs)
print(f"å‰å‘ä¼ æ’­ç¼–è¯‘: {time.time() - t0:.2f}s")
print(f"è¾“å‡º policy_logits å½¢çŠ¶: {output.policy_logits.shape}")
print(f"è¾“å‡º value å½¢çŠ¶: {output.value.shape}")
print("âœ… ç½‘ç»œå‰å‘ä¼ æ’­æ­£å¸¸")

print("\n" + "=" * 60)
print("Step 6: æµ‹è¯• MCTS (å•æ­¥)")
print("=" * 60)

import mctx

def recurrent_fn(params, rng_key, action, embedding):
    next_state, reward, logits, value = network.apply(
        params, embedding, action.astype(jnp.int32),
        method=network.recurrent_inference
    )
    return mctx.RecurrentFnOutput(
        reward=reward,
        discount=jnp.ones_like(reward),
        prior_logits=logits,
        value=value,
    ), next_state

@jax.jit
def mcts_step(params, obs, legal_mask, key):
    output = network.apply(params, obs)
    root = mctx.RootFnOutput(
        prior_logits=output.policy_logits,
        value=output.value,
        embedding=output.hidden_state,
    )
    policy_output = mctx.gumbel_muzero_policy(
        params=params,
        rng_key=key,
        root=root,
        recurrent_fn=recurrent_fn,
        num_simulations=8,  # å¾ˆå°çš„å€¼
        invalid_actions=~legal_mask,
        max_num_considered_actions=8,
    )
    return policy_output

print("ç¼–è¯‘ MCTS (16 å±€å¹¶è¡Œ, 8 æ¬¡æ¨¡æ‹Ÿ)...")
t0 = time.time()
policy_out = mcts_step(
    train_state.params,
    obs,
    states.legal_action_mask,
    key,
)
policy_out.action.block_until_ready()
print(f"MCTS ç¼–è¯‘: {time.time() - t0:.2f}s")
print(f"é€‰æ‹©çš„åŠ¨ä½œ: {policy_out.action[:5]}")
print("âœ… MCTS æ­£å¸¸")

print("\n" + "=" * 60)
print("Step 7: æµ‹è¯• lax.scan (å¤šæ­¥)")
print("=" * 60)

@jax.jit
def multi_step(params, state, key):
    """æ‰§è¡Œ 5 æ­¥è‡ªç©"""
    v_observe = jax.vmap(env.observe)
    v_step = jax.vmap(env.step)
    v_init = jax.vmap(env.init)
    
    def step_fn(state, key):
        obs = v_observe(state)
        output = network.apply(params, obs)
        
        # ç®€å•ç­–ç•¥ï¼šè´ªå¿ƒé€‰æ‹©
        logits = jnp.where(state.legal_action_mask, output.policy_logits, -1e9)
        action = jnp.argmax(logits, axis=-1)
        
        next_state = v_step(state, action)
        return next_state, None
    
    final_state, _ = jax.lax.scan(
        step_fn,
        state,
        jax.random.split(key, 5),  # 5 æ­¥
    )
    return final_state

print("ç¼–è¯‘ lax.scan (5 æ­¥, 16 å±€å¹¶è¡Œ)...")
t0 = time.time()
final = multi_step(train_state.params, states, key)
final.board.block_until_ready()
print(f"lax.scan ç¼–è¯‘: {time.time() - t0:.2f}s")
print("âœ… lax.scan æ­£å¸¸")

print("\n" + "=" * 60)
print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
print("=" * 60)
print("\nå¦‚æœè¿™ä¸ªè„šæœ¬èƒ½æ­£å¸¸è¿è¡Œå®Œæˆï¼Œè¯´æ˜å„ä¸ªç»„ä»¶éƒ½æ²¡é—®é¢˜ã€‚")
print("é—®é¢˜å¯èƒ½åœ¨äº selfplay_fn ä¸­åµŒå¥—äº† MCTS + lax.scanï¼Œ")
print("å¯¼è‡´è®¡ç®—å›¾è¿‡äºå¤æ‚ï¼Œç¼–è¯‘æ—¶é—´è¿‡é•¿ã€‚")
print("\nå»ºè®®:")
print("1. ç»§ç»­ç­‰å¾… train.py çš„ç¼–è¯‘å®Œæˆï¼ˆå¯èƒ½éœ€è¦ 30-60 åˆ†é’Ÿï¼‰")
print("2. æˆ–è€…è€ƒè™‘ä½¿ç”¨ AlphaZero æ–¹å¼ï¼ˆçœŸå®ç¯å¢ƒæ¨¡æ‹Ÿï¼‰è€Œä¸æ˜¯ MuZeroï¼ˆå­¦ä¹ çš„åŠ¨æ€æ¨¡å‹ï¼‰")
