# ============================================================================
# ZeroForge - 中国象棋 Gumbel MuZero 训练配置
# ============================================================================

# 实验名称
experiment_name: "xiangqi_muzero_v1"

# 随机种子
seed: 42

# ============================================================================
# 环境配置
# ============================================================================
env:
  # 历史步数
  num_history_steps: 16
  # 最大游戏步数
  max_steps: 200
  # 无吃子最大步数 (和棋判定)
  max_no_capture_steps: 120

# ============================================================================
# 网络配置
# ============================================================================
network:
  # 隐藏层维度
  hidden_dim: 256
  # 观察通道数 (自动计算: (16+1)*14+2 = 240)
  observation_channels: 240
  # 动作空间大小
  action_space_size: 2086
  
  # Representation Network
  repr_blocks: 8
  
  # Dynamics Network
  dyn_blocks: 4
  
  # Prediction Network
  pred_blocks: 4
  
  # 价值和奖励分布 (0=标量, >0=分类)
  value_support_size: 0
  reward_support_size: 0
  
  # Drop Path (正则化)
  drop_path_rate: 0.1

# ============================================================================
# MCTS 配置
# ============================================================================
mcts:
  # 模拟次数
  num_simulations: 800
  # Gumbel 采样的候选动作数
  max_num_considered_actions: 16
  # Gumbel 缩放因子
  gumbel_scale: 1.0
  # 折扣因子
  discount: 0.997
  # 温度 (训练时用于策略目标)
  temperature: 1.0
  
  # Dirichlet 噪声 (训练时增加探索)
  dirichlet_alpha: 0.3
  dirichlet_fraction: 0.25

# ============================================================================
# 训练配置
# ============================================================================
training:
  # 总训练步数
  num_training_steps: 1000000
  
  # 批次大小 (每个设备)
  batch_size: 256
  
  # 学习率
  learning_rate: 0.0002
  lr_warmup_steps: 1000
  lr_decay_steps: 100000
  
  # 权重衰减
  weight_decay: 0.0001
  
  # 梯度裁剪
  gradient_clip: 1.0
  
  # MuZero 展开步数
  unroll_steps: 5
  td_steps: 10
  
  # 损失权重
  policy_loss_weight: 1.0
  value_loss_weight: 0.25
  reward_loss_weight: 1.0
  
  # EMA (指数移动平均)
  use_ema: true
  ema_decay: 0.999

# ============================================================================
# Replay Buffer 配置
# ============================================================================
replay_buffer:
  # 容量 (位置数)
  capacity: 100000
  # 最小采样所需的数据量
  min_size: 1000
  
  # 优先级采样
  alpha: 0.6  # 优先级指数
  beta: 0.4   # 重要性采样指数
  beta_increment: 0.001

# ============================================================================
# 自我对弈配置
# ============================================================================
self_play:
  # 并行游戏数
  num_parallel_games: 64
  # Actor 数量
  num_actors: 4
  # 每次更新前的对局数
  games_per_training_step: 1
  
  # 数据增强
  use_mirror_augmentation: true
  mirror_probability: 0.5

# ============================================================================
# 评估配置
# ============================================================================
evaluation:
  # 评估间隔 (步数)
  eval_interval: 5000
  # 每次评估的对局数
  num_eval_games: 100
  
  # 模型更新策略:
  # - "always": Gumbel MuZero 推荐，始终使用最新模型
  # - "threshold": 传统 AlphaZero 方式，需要胜率超过阈值
  update_strategy: "always"
  
  # 仅当 update_strategy="threshold" 时使用
  win_threshold: 0.55
  
  # ELO 配置 (用于监控，不影响模型更新)
  elo_k_factor: 32.0
  elo_k_factor_min: 16.0
  elo_initial_rating: 1500.0

# ============================================================================
# 日志和检查点配置
# ============================================================================
logging:
  # 日志目录
  log_dir: "logs"
  # 控制台输出间隔
  console_interval: 100
  # TensorBoard 日志间隔
  tensorboard_interval: 10

checkpoint:
  # 检查点目录
  checkpoint_dir: "checkpoints"
  # 保存间隔 (步数)
  save_interval: 1000
  # 最多保留数量
  max_to_keep: 5
  # 永久保留间隔
  keep_period: 10000

# ============================================================================
# 分布式训练配置
# ============================================================================
distributed:
  # 是否使用多 GPU
  multi_gpu: true
  # 数据并行策略
  strategy: "data_parallel"
